Agent-Based LLM Evaluator: Leveraging LLMs to Design an Advanced Agent-Based Evaluator for Deep Learning Models.
=

ABSTRACT
-
An agent-based evaluator framework, termed the agent-based Large Language Models (LLMs) evaluator (ABLE), is proposed to automate deep learning model assessment through natural language interaction and structured database integration. Built on the Dify platform, ABLE leverages large language models to interpret user queries, retrieve evaluation data, and generate graphical or document-based outputs. A MySQL–Navicat database enables traceable management of model attributes and metrics, and ChatFlow facilitates context-aware interactions. The effectiveness of the evaluator is validated by using YOLOv1 to YOLOv13, with a comparative LLM performance analysis with DeepSeek V3 and Qwen-Turbo-Latest. ABLE demonstrates extensibility, interpretability, and adaptability and offers a generalizable paradigm for intelligent and user-centric model evaluation across scientific and industrial domains.

Keywords:
-
Agent-Based Evaluation; Large Language Models (LLMs); YOLO Object Detection; Conversational AI Framework; Structured Model Benchmarking




<ins>"Agent Based LLMs Evaluator.yml" can be imported as DSL file in the Dify framework, and the running results are as follows:</sub>

Q1：Check the release times of YOLOv1 to YOLOv13 versions and generate a bar chart.
-

https://github.com/user-attachments/assets/ffcd5b93-306d-410d-99ed-4a97cf8b9da8




<img width="2160" height="1516" alt="Fig 1" src="https://github.com/user-attachments/assets/e1791ebf-dd03-4d7e-9854-5e645c14d064" />
Fig. 1 The agent-based LLM evaluator consists of three key modules. The 'Database Integration' module, which is shown in green, stores and organizes model data for evaluation and uses MySQL for management. The 'Agentic AI Applications' module, which is shown in red and is based on the Dify architecture, enables agent–database interaction for data query and update. The 'Agent Interaction Model Design' module, which is shown in purple, designs chat-based evaluation processes and generates visualizations and data outputs to enhance the user experience.









<img width="1107" height="650" alt="image" src="https://github.com/user-attachments/assets/5ce817b2-2e27-444b-a090-b77a5cb5c997" />
Fig. 2 Integrated MySQL–Navicat database architecture for the structured management of deep learning model metadata and the evaluation of parameters in ABLE


<img width="1108" height="651" alt="image" src="https://github.com/user-attachments/assets/1400f930-60fa-4f12-849d-7354ee1d420e" />
Fig. 3 Modular architecture of agent-based AI solutions within the ABLE framework: Enabling natural language interaction, intelligent query processing, and customized evaluation outputs for deep learning models


<img width="1108" height="541" alt="image" src="https://github.com/user-attachments/assets/2ce70dbc-aefa-458c-8718-760157f7f36b" />
Fig. 4 Conversational interaction architecture that is based on ChatFlow: A modular agent design for deep learning model evaluation in the ABLE framework

<img width="932" height="599" alt="image" src="https://github.com/user-attachments/assets/071f8cfb-c84d-4093-a809-31da9f430e48" />

Fig. 5 Structural overview of the agent-based LLM evaluator, which comprises three interrelated modules: (1) graphical visualization for intuitive performance interpretation, (2) a comprehensive case study of YOLO models from v1 to v13, and (3) a benchmarking methodology that is designed for comparing model metrics and literature standards. This modular design enhances analytical depth, user interactivity, and evaluation generalizability across various deep learning architectures.

<img width="1107" height="792" alt="image" src="https://github.com/user-attachments/assets/eb8f382f-f7c3-4f07-8373-909755ec4b1c" />
Fig. 6 Bar chart generated by the agent evaluator

<img width="1107" height="792" alt="image" src="https://github.com/user-attachments/assets/9fbfedf6-ce80-4c5a-ad0b-9e765b37141a" />
Fig. 7 Evolutionary trajectory of YOLO architectures from v1 to v13: Structural advancements and performance trends



Q2: Check the data of YOLO version and analyze the overall development of YOLO.
-



https://github.com/user-attachments/assets/0cadf894-9e18-4164-8747-ccd45c2c8f93









https://github.com/user-attachments/assets/6a7e3827-7b06-458c-892d-5e51d1e0699a











<img width="1107" height="740" alt="image" src="https://github.com/user-attachments/assets/bc0c3cb9-9d04-4d64-8a1f-28cb7e791bc4" />
Fig. 8 Comparative evaluation of LLM-based agents for YOLO model analysis: Reasoning depth, interpretability, and response effectiveness


Q3:Please evaluate these 10 papers related to deep learning YOLO series models, why are they cited so much? Meanwhile, provide some suggestions on how to design YOLO models that will attract widespread attention from researchers?
-





https://github.com/user-attachments/assets/2f96c56f-d5b6-4c67-82b3-968bf316b77a







https://github.com/user-attachments/assets/55524bd9-11a4-48aa-b19f-9ffae62cbff8







<img width="1111" height="682" alt="image" src="https://github.com/user-attachments/assets/43426b44-2d97-4992-9c84-aad74cce82ba" />
Fig. 9 Comparative token utilization (a) and inference time of DeepSeek V3 and Qwen-Turbo-Latest across multi-step agentic evaluation tasks (b)


Conclusion
=
In this study, an advanced agent-based evaluator—ABLE (Agent-Based LLMs Evaluator)—has been developed to facilitate intelligent, scalable, and user-centric evaluation of deep learning models. Built upon a modular architecture, ABLE integrates a MySQL-Navicat database for structured model data management, the Dify framework for agent-based interaction, and a Chatflow-driven interface for dynamic conversational evaluation. Through this tripartite design, a seamless pipeline has been constructed in which structured data, LLMs, and user prompts converge to produce interpretable and customizable outputs. The system has been validated through a comprehensive case study involving YOLO models from YOLOv1 to YOLOv13, demonstrating its capacity to extract, visualize, and benchmark architectural and performance characteristics across model versions. In addition, the ability to automate visualizations and document outputs in DOCX, PDF, and HTML formats has been realized, thereby reducing the technical barrier for model assessment.

A comparative evaluation of two LLMs-DeepSeek V3 and Qwen-Turbo-Latest-has been conducted to assess their effectiveness when deployed within agentic evaluation workflows. It has been observed that DeepSeek V3 exhibits superior semantic density and coherence in complex analytical scenarios, while Qwen-Turbo-Latest offers lower latency and more efficient handling of simpler tasks. These findings underscore the importance of model selection and task alignment in evaluation-oriented agent design. Furthermore, the open-source and extensible nature of ABLE ensures compatibility with emerging LLMs, evaluation schemas, and domain-specific use cases. The combined integration of intelligent reasoning, structured data processing, and multimodal output generation establishes ABLE as a generalizable platform for automated deep learning model evaluation.



